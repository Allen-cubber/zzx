---
title: 一、信息论基础
date: 2025-09-06T11:00:00+08:00
tags:
  - 信息论
  - 大三上
summary: 信息论是由美国科学家香农在1948年通过其论文《通信的数学理论》奠定的理论基础。
pdf_url: https://cdn.jsdelivr.net/gh/allen-cubber/zzx-blog-assets@v1.0/xinxilunjichu.pdf
---
### 📚 信息论与编码 (第四章 信息论基础) 学习笔记

### 4.1 引言 💡

信息论是由美国科学家<font color="orange">香农</font>在1948年通过其论文《通信的数学理论》奠定的理论基础。

#### 消息与信息

*   **消息** ✉️: 由符号、文字、数字、语音或图像等组成的序列，是信息的物理载体。
*   **信息** 🧠: 是消息所包含的<font color="orange">内涵</font>。信息的本质是用来<font color="orange">消除不确定性</font>的东西。
    *   **形式化**: 通信的基本任务是在接收端从形式上恢复出发送端发出的消息，不关心对消息内容的理解。
    *   **不确定性**: 在收到消息前，消息的内容对接收者来说是随机的、不确定的。通信的作用就是通过传递消息，消除或部分消除这种不确定性。
    *   <font color="orange">信息量</font>在数量上就等于获得消息前后“不确定性”的消除量。

---

### 4.2 信息的度量 📐

#### 信息量的概念

1.  **某消息的信息量 = 获得该消息后不确定性的消除量**。
2.  不确定性与事件发生的<font color="orange">概率</font>有关。事件发生的概率越小，其不确定性就越大，一旦发生，所提供的信息量也越大。
3.  信息量的度量应满足<font color="orange">可加性</font>。即两个独立事件同时发生的信息量，等于它们各自信息量之和。

#### 4.2.1 离散信源信息的度量 (自信息)

*   **自信息定义**: 对于一个离散消息 $x_i$ ，其信息量（自信息）定义为：
    $
    \boxed{I(x_i) = \log \frac{1}{P(x_i)} = -\log P(x_i)}
    $
    其中 $P(x_i)$ 为消息 $x_i$ 发生的概率。

*   **单位**: 信息量的单位与对数的底有关。
    *   底为2: <font color="orange">**比特 (bit)**</font> ✅ (默认单位)
    *   底为e: 奈特 (nit)
    *   底为10: 哈特 (hart)

*   **📝 示例**:
    已知某信源的概率场为:
    $X:$
    $p(X): [3/8, 1/4, 1/4, 1/8]$
    各符号统计独立。
    1.  **计算各符号信息量**:
        $I(0) = -\log_2(3/8) \approx 1.415 \ \text{bit}$
        $I(1) = -\log_2(1/4) = 2 \ \text{bit}$
        $I(2) = -\log_2(1/4) = 2 \ \text{bit}$
        $I(3) = -\log_2(1/8) = 3 \ \text{bit}$
    2.  **计算符号序列 S="113200" 的信息量**:
        由于各符号独立，信息量可加。
        $I(S) = I(1) + I(1) + I(3) + I(2) + I(0) + I(0)$
        $I(S) = 2 + 2 + 3 + 2 + 1.415 + 1.415 = 11.83 \ \text{bit}$

#### 4.2.2 离散信源的平均信息量 (信源的熵)

*   **熵的定义**: 熵是信源输出的<font color="orange">**平均信息量**</font>，表示信源在统计意义上每个符号所含信息量的平均值。
*   **熵的公式**:
    $
    \boxed{H(X) = \sum_{i=1}^{N} p(x_i)I(x_i) = -\sum_{i=1}^{N} p(x_i)\log p(x_i)}
    $
*   **单位**: <font color="orange">**比特/符号**</font>。

*   **🧠 核心思想**: 熵描述了一个信源的<font color="orange">**平均不确定性**</font>。熵越大的信源，其输出同样个数的符号，所含的信息量就越大。

*   **⚙️ 示例**:
    求上一示例中信源的熵。
    $H(X) = -[p(0)\log p(0) + p(1)\log p(1) + p(2)\log p(2) + p(3)\log p(3)]$
    $H(X) = -[\frac{3}{8}\log_2\frac{3}{8} + \frac{1}{4}\log_2\frac{1}{4} + \frac{1}{4}\log_2\frac{1}{4} + \frac{1}{8}\log_2\frac{1}{8}]$
    $H(X) = \frac{3}{8}(1.415) + \frac{1}{4}(2) + \frac{1}{4}(2) + \frac{1}{8}(3) \approx 1.906 \ \text{比特/符号}$

#### 4.2.3 离散信源的最大熵定理

*   **定理**: 当离散信源X中所有符号<font color="orange">**等概率**</font>分布时，其熵 $H(X)$ 取最大值。
*   **最大熵公式**:
    $
    H_{\max} = \log_2 N
    $
    其中 N 是信源中不同符号的个数。
*   **物理意义**: 当信源取等概分布时，其不确定性最大。
*   **应用**: 如果想让每个符号/码元平均携带最大的信息量，可以对信源进行变换，使其变换后的符号集具有等概或近似等概的特性。

#### 4.2.4 联合熵与条件熵

*   **联合熵 $H(XY)$**:
    描述一个联合信源 $XY$ 的平均不确定性。
    $
    H(XY) = -\sum_{i=1}^{M}\sum_{j=1}^{N} p(x_i, y_j)\log p(x_i, y_j)
    $
    *   **重要性质**: 当 X 和 Y <font color="orange">**统计独立**</font>时， $H(XY) = H(X) + H(Y)$。

*   **条件熵 $H(Y|X)$**:
    在已知随机变量 X 的条件下，随机变量 Y 仍具有的不确定性（平均信息量）。
    $
    H(Y|X) = -\sum_{i=1}^{M}\sum_{j=1}^{N} p(x_i, y_j)\log p(y_j|x_i)
    $
    *   **重要性质**: $H(Y|X) \le H(Y)$。这表明，<font color="orange">**知识总是有益的**</font>，知道一个变量 X 的信息，总会使得另一个相关变量 Y 的不确定性减小或不变。

---

### **4.3 节 离散信道及容量**

#### **4.3.1 信道模型 📡**

信道是信息传输的通道。我们可以从数学上对其进行建模。

*   **信道输入 (X):** 一个符号集合，记作 $ \{X : x_i, i = 1, 2, ..., M\} $
*   **信道输出 (Y):** 一个符号集合，记作 $ \{Y : y_j, j = 1, 2, ..., N\} $

<font color="orange">信道</font>的概念可以很广泛：
1.  **狭义信道：** 物理传输介质，如无线信道、光纤。
2.  **广义信道：** 任何经历“输入-处理-输出”过程的系统，如数据存储与读取（记录与重放）。

##### **离散信道模型**
我们使用<font color="orange">转移概率</font>（或传递概率）来描述信道的统计特性。

*   **有记忆信道：** 输出不仅与当前输入有关，还与过去的输入有关。其转移概率为：
    $ p(y_j^{(k)} / x^{(k)}x^{(k-1)}...x^{(k-n)}) $
*   **无记忆信道：** 输出仅与当前的输入有关。这是本课程讨论的重点。

对于<font color="orange">离散无记忆信道</font>，其特性完全由一个<font color="orange">转移概率矩阵</font>来描述。



---

#### **概率矩阵 📊**

##### **1. 离散无记忆信道的<font color="orange">转移概率矩阵</font>**
这个矩阵描述了当输入为 $x_i$ 时，输出为 $y_j$ 的概率。

$\boxed{ [P(Y/X)] = \begin{bmatrix} p(y_1/x_1) & p(y_2/x_1) & \cdots & p(y_N/x_1) \\\\ p(y_1/x_2) & p(y_2/x_2) & \cdots & p(y_N/x_2) \\\\ \vdots & \vdots & \ddots & \vdots \\\\ p(y_1/x_M) & p(y_2/x_M) & \cdots & p(y_N/x_M) \end{bmatrix} }$

*   其中 $p(y_j/x_i)$ 表示：当发送符号 $x_i$ 时，接收到符号 $y_j$ 的概率。

##### **2. 离散无记忆信道的<font color="orange">后验概率矩阵</font>**
这个矩阵描述了当接收到 $y_j$ 时，原始发送符号为 $x_i$ 的概率。

$[P(X/Y)] = \begin{bmatrix} p(x_1/y_1) & p(x_2/y_1) & \cdots & p(x_M/y_1) \\\\ p(x_1/y_2) & p(x_2/y_2) & \cdots & p(x_M/y_2) \\\\ \vdots & \vdots & \ddots & \vdots \\\\ p(x_1/y_N) & p(x_2/y_N) & \cdots & p(x_M/y_N) \end{bmatrix}$

*   其中 $p(x_i/y_j)$ 表示：当收到符号 $y_j$ 时，发送符号是 $x_i$ 的概率。

##### **示例：二元离散无记忆信道**
假设一个二元信道，发送 "0" 和 "1"，正确接收的概率为 0.99，错误接收的概率为 0.01。

*   $p(0/0) = 0.99$ (发送0，收到0)
*   $p(1/1) = 0.99$ (发送1，收到1)
*   $p(1/0) = 0.01$ (发送0，收到1，即错误)
*   $p(0/1) = 0.01$ (发送1，收到0，即错误)

其<font color="orange">转移概率矩阵</font>为：
$[p(Y/X)] = \begin{bmatrix} 0.99 & 0.01 \\\\ 0.01 & 0.99 \end{bmatrix}$

---

#### **4.3.2 互信息量 💡**

<font color="orange">互信息量</font>衡量的是，当接收到一个符号 $y_j$ 后，我们获得了多少关于原始发送符号 $x_i$ 的信息。

*   收到 $y_j$ 后，关于 $x_i$ 的不确定性（条件自信息量）为：
    $I(x_i/y_j) = \log \frac{1}{p(x_i/y_j)}$
*   **定义：** <font color="orange">互信息量</font>是先验不确定性 $I(x_i)$ 与后验不确定性 $I(x_i/y_j)$ 之差。
    $ \boxed{ I(x_i;y_j) = I(x_i) - I(x_i/y_j) } $
*   **物理意义：** 收到 $y_j$ 后，关于 $x_i$ 的不确定性的<font color="orange">消除量</font>。

##### **互信息量的性质**

1.  **对称性:** 知道 $y_j$ 后获得的关于 $x_i$ 的信息量，等于知道 $x_i$ 后获得的关于 $y_j$ 的信息量。
    $ I(x_i;y_j) = I(y_j;x_i) $
    **推导:**
    $ I(x_i;y_j) = \log\frac{1}{p(x_i)} - \log\frac{1}{p(x_i/y_j)} = \log\frac{p(x_i/y_j)}{p(x_i)} = \log\frac{p(x_i, y_j)}{p(x_i)p(y_j)} = \log\frac{p(y_j/x_i)}{p(y_j)} = I(y_j;x_i) $

2.  **其他性质:**
    *   若 $p(x_i/y_j) = 1$ (收到 $y_j$ 就百分百确定发送的是 $x_i$) ➡️ $I(x_i;y_j) = I(x_i)$ (不确定性完全消除)。
    *   若 $p(x_i/y_j) = p(x_i)$ (收到 $y_j$ 对判断 $x_i$ 毫无帮助) ➡️ $I(x_i;y_j) = 0$ (未获得任何信息)。
    *   若 $p(x_i) < p(x_i/y_j) < 1$ ➡️ $0 < I(x_i;y_j) < I(x_i)$ (获得了部分信息)。
    *   若 $0 < p(x_i/y_j) < p(x_i)$ ➡️ $I(x_i;y_j) < 0$ (收到 $y_j$ 反而增加了对 $x_i$ 的不确定性，即被误导了)。

##### **平均互信息量**
<font color="orange">平均互信息量</font>是互信息量 $I(x_i;y_j)$ 在所有可能的输入输出对 $(x_i, y_j)$ 上的统计平均值。

*   **定义:**
    $ \boxed{ I(X;Y) = \sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i, y_j) I(x_i;y_j) } $
*   **理解:** 信道每传输一个符号，在接收端平均获得的信息量。
*   **非负性:** 平均互信息量总是大于等于零。
    $ I(X;Y) \ge 0 $
    这意味着从统计上来说，接收符号总是有利于（或至少不损害）我们对发送符号的判断。

---

#### **4.3.3 熵与平均互信息量的关系 🔗**

平均互信息量与熵函数之间存在紧密的关系，这些关系是信息论的核心。

*  $ I(X;Y) = H(X) - H(X/Y) $ (信源熵减去疑义度)
*  $ I(X;Y) = H(Y) - H(Y/X) $ (信宿熵减去噪声熵)
*  $ H(X/Y) \le H(X) $ (知道Y后，X的不确定性减小)
*  $ H(Y/X) \le H(Y) $ (知道X后，Y的不确定性减小)
* $ H(XY) \le H(X) + H(Y) $ (联合熵小于等于熵之和)
![](content/notes/信息论与编码/1.信息论基础/Pastedimage20250913150043.png)
##### **两种极端情况**

**1. 当信源X与Y统计独立时**

*   $ I(X;Y) = 0 $ (互信息量为0)
*   $ H(X/Y) = H(X) $
*   $ H(Y/X) = H(Y) $
*   $ H(XY) = H(X) + H(Y) $ (联合熵等于熵之和)
*   **结论:** 一个符号不能提供有关另一个符号的任何信息。

**2. 当两个信源相关时**

*   $ H(X/Y) < H(X) $
*   $ H(Y/X) < H(Y) $
*   $ H(XY) < H(X) + H(Y) $
*   $ I(X;Y) \le H(X) $
*   **结论 (Venn图解释):**
    *   **平均互信息量** $I(X;Y)$ 是两个信源熵 $H(X)$ 和 $H(Y)$ <font color="orange">重叠</font>的部分。
    *   **条件熵** $H(X/Y)$ 是 $H(X)$ 中<font color="orange">不重叠</font>的部分。
    *   **联合熵** $H(XY)$ 是两个信源熵<font color="orange">并集</font>的区域。

---

#### **4.3.4 离散信道的容量 📈**

**信道容量 (Channel Capacity)** 是信息论中的一个核心概念，它代表了一个信道在理论上能够无差错传输的最大信息速率。

*   **符号传输速率:** $R_s$ (单位: 符号/秒)
*   **系统的信息速率:** $R_i = I(X;Y) \times R_s$ (单位: 比特/秒)
    $ R_i = \left[ \sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i)p(y_j/x_i) \log \frac{p(y_j/x_i)}{p(y_j)} \right] \times R_s $

*   **定义：<font color="orange">信道容量</font> (C)** 是在所有可能的信源输入概率分布 $p(x_i)$ 下，系统信息速率 $R_i$ 的最大值。
    $ \boxed{ C = \max_{p(x_i), i=1,...,M} R_i = \max_{p(x_i), i=1,...,M} [I(X;Y) \times R_s] } $

*   **关键点:**
    *   信道容量由<font color="orange">信道特性</font> (转移概率矩阵) 和<font color="orange">信源统计特性</font> (输入概率分布) 共同决定。
    *   一旦信道特性确定，其容量就只取决于信源的统计特性。

*   **<font color="orange">匹配信源</font>:** 能够使信道的信息传输速率达到其容量的那个特定概率分布的信源。我们的目标就是找到这个“最佳”的信源分布。

---

#### **如何求解<font color="orange">匹配信源</font> ⚙️**

**问题:** 已知信道转移概率 $p(y_j/x_i)$，求能使 $I(X;Y)$ 达到最大的信源分布 $p(x_i)$。
**方法:** 使用拉格朗日乘数法求解约束条件下的极值问题。

##### **求解步骤 ✍️**

这是一个标准化的四步流程：

**步骤 (1): 解方程组，求辅助变量 $\beta_j$**
解以下 M 个方程组成的方程组，求出 N 个 $\beta_j$ 值。
$ \sum_{j=1}^{N} p(y_j/x_i) \log p(y_j/x_i) = \sum_{j=1}^{N} p(y_j/x_i) \beta_j, \quad i=1, 2, ..., M $

**步骤 (2): 求最大平均互信息量 $I_m$**
将上一步求得的 $\beta_j$ 代入下式。
$ I_m = \log \left( \sum_{j=1}^{N} 2^{\beta_j} \right) $

**步骤 (3): 求接收符号的概率 $p(y_j)$**
$ p(y_j) = 2^{\beta_j - I_m}, \quad j=1, 2, ..., N $

**步骤 (4): 解方程组，求出匹配信源的概率分布 $p_m(x_i)$**
求解下面的线性方程组，得到最终的匹配信源分布。
$ p(y_j) = \sum_{i=1}^{M} p(y_j/x_i) p_m(x_i), \quad j=1, 2, ..., N $

> **💡 注意点:** 在求解步骤 (4) 的过程中，如果出现某个 $p_m(x_i) < 0$ 的情况，说明最优解在边界上。此时应令该 $p_m(x_i) = 0$，然后从步骤 (1) 开始重新求解。

---

#### **4.3.5 离散无记忆<font color="orange">对称信道</font>**

<font color="orange">对称信道</font>是一种特殊的信道，其转移概率矩阵具有高度的对称性。

*   **定义:** 转移概率矩阵的<font color="orange">每一行</font>都是相同元素集合的一个排列，并且<font color="orange">每一列</font>也是相同元素集合的一个排列。

##### **对称信道的特性**

1.  **条件熵 $H(Y/X)$ 与信源无关：** 条件熵（噪声熵）仅由信道本身的特性决定，与输入信源的概率分布无关。
    $ H(Y/X) = - \sum_{j=1}^{N} p(y_j/x_i) \log p(y_j/x_i) $  (对任意 $i$ 都成立)

2.  **等概输入对应等概输出：** 如果输入信道的信源符号是<font color="orange">等概率</font>分布的 ($p(x_i) = 1/M$)，那么信道的输出符号也必然是<font color="orange">等概率</font>分布的 ($p(y_j) = 1/N$)。

##### **对称信道的信道容量**

对于对称信道，求解信道容量变得非常简单。

*   $ C = \max [H(Y) - H(Y/X)] \times R_s $
*   由于 $H(Y/X)$ 是一个与信源无关的常数，最大化 $I(X;Y)$ 就等价于最大化 $H(Y)$。
*   当输出符号等概率分布时，$H(Y)$ 达到最大值 $\log N$。
*   根据特性2，要使输出等概率，输入必须是<font color="orange">等概率</font>的。

**结论:** 🧠
对于<font color="orange">离散无记忆对称信道</font>，其<font color="orange">匹配信源</font>一定是<font color="orange">等概率分布</font>的信源。
其信道容量为：
$ \boxed{ C = (\log N - H(Y/X)) \times R_s } $

> **🎯 实际应用:** 对于非等概的信源，可以通过<font color="orange">信源编码</font>（如等长编码）技术，将其变换为近似等概的新信源，从而更有效地利用对称信道。

---
### 🌟 本章学习总结

本章作为信息论的基础，核心是围绕“<font color="orange">**不确定性**</font>”的度量展开的。

1.  **从单个事件出发** ➡️ 我们学习了用<font color="orange">**自信息**</font> $I(x)$ 来量化一个事件发生所带来的信息量，它与概率成反比。
2.  **扩展到整个信源** ➡️ 我们引入了<font color="orange">**熵**</font> $H(X)$ 的概念，它衡量了一个信源输出符号的平均信息量，也就是信源的平均不确定性。我们还知道了，<font color="orange">**等概率分布**</font>的信源熵最大。
3.  **考虑信源间的关系** ➡️ 通过<font color="orange">**联合熵**</font> $H(XY)$ 和<font color="orange">**条件熵**</font> $H(Y|X)$，我们能够量化多个信源之间以及它们相互提供信息后不确定性的变化。
4.  **研究信息在信道中的传递** ➡️ <font color="orange">**互信息**</font> $I(X;Y)$ 成为了关键，它衡量了信道输入和输出之间的关联程度，即通过信道成功传递的平均信息量。
5.  **衡量信道的终极能力** ➡️ 最后，我们定义了<font color="orange">**信道容量**</font> $C$，它是信道能够达到的理论信息传输速率上限。对于给定的信道，通过寻找一个“<font color="orange">**匹配信源**</font>”，就可以实现这个最大速率。

