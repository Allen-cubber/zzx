---
title: 信源编码的基本方法
date: 2025-09-26T10:00:00+08:00
tags:
  - 大三上
  - 信息论
summary: 信源编码的本质是在效率（更短的码长）和可靠性（唯一可译/低失真）之间寻找最佳平衡，而信息论为我们提供了衡量这种平衡的理论基础和实现最优编码的指导方法。
---
信源编码的根本目的是<font color="orange">提高信息传输的有效性</font>。这可以通过以下几种方式实现：

1.  **消除冗余** 🗑️：去除消息中多余的、可预测的成分，使传输的符号尽可能独立。例如语音、图像信号的压缩。
2.  **信息最大化** 📈：通过编码使传输的符号尽可能以<font color="orange">等概率</font>分布的形式出现，这样每个符号携带的信息量达到最大值。
3.  **不等长编码** 🔢：让出现概率大的符号用较短的码元序列表示，概率小的符号用较长的码元序列表示，从而在统计上缩短平均码长。
4.  **高效有损编码** 🎯：在允许一定失真的条件下，实现尽可能高的编码效率。

##### **核心结论**

*   **结论1 (有记忆信源)** 🧠：有记忆信源的冗余度主要体现在信源符号之间的<font color="orange">相关性</font>中。去除这种相关性，可以使信源变得（或接近）无记忆，其熵会增大，信息潜力得到释放。
    *   数据压缩的基本途径：通过减弱相关性，使有记忆信源趋向于无记忆信源，熵从 $H_\infty$ 增大到 $H(X)$，再通过增强均匀性，最终达到最大熵 $H_0$。
    $$
    H_∞ \le ... \le H(X_N|X_1...X_{N-1}) \le ... \le H(X_2|X_1) \le H(X) \le H_0=\log q
    $$

*   **结论2 (无记忆信源)** 🎲：离散无记忆信源的冗余度主要体现在符号概率的<font color="orange">非均匀分布</font>中。改变其概率分布，使其接近等概分布，熵也会增大。

#### **📚 离散信源的分类**

1.  **按记忆性分类**:
    *   **离散无记忆信源 (DMS)**：发出的各个符号相互独立，没有统计关联性。每个符号的出现概率是其自身的先验概率。
    *   **离散有记忆信源**：发出的各个符号之间不是相互独立的，出现概率存在关联。

2.  **按输出符号形式分类**:
    *   **发出单个符号的离散信源**：每次只发出一个符号代表一个消息。
    *   **发出符号序列的离散信源**：每次发出一组（两个或以上）符号序列代表一个消息。

将这两种分类结合，可以得到四种离散信源：
*   (1) 单符号无记忆离散信源
*   (2) 符号序列无记忆离散信源
*   (3) 单符号有记忆离散信源
*   (4) 符号序列有记忆离散信源

#### **🔢 4.5.1 离散无记忆信源 (DMS) 及其扩展**

##### **1. 单个符号的离散信源**

每次只发出一个符号代表一个消息，其概率空间可以表示为：
$$
\begin{bmatrix} X \\\\ P \end{bmatrix} = \begin{bmatrix} S_1 & S_2 & \cdots & S_L \\\\ p(S_1) & p(S_2) & \cdots & p(S_L) \end{bmatrix}
$$
其中符号集为 $X = \{S_1, S_2, \dots, S_L\}$，且满足：
$$
p(S_i) \ge 0, \quad \sum_{i=1}^{L} p(S_i) = 1
$$

##### **2. 发出符号序列的离散信源 (扩展信源)**

当信源每次输出一个长度为 $N$ 的符号序列（随机矢量）$\bar{X}=(X_1, X_2, \dots, X_N)$ 时，就构成了一个<font color="orange">扩展信源</font>。

*   **定义**：如果组成序列的各个随机变量 $X_i$ 相互独立，即满足：
    $$
    p(X_{i_1}X_{i_2}\cdots X_{i_N}) = p(X_{i_1})p(X_{i_2})\cdots p(X_{i_N})
    $$
    则称该信源为**离散无记忆N次扩展信源**，记为 $X^N$。

*   **扩展信源的熵**：N次扩展信源的熵是单个符号信源熵的N倍。
    $$
    \boxed{H(X^N) = NH(X)}
    $$
    这意味着，一个由N个符号组成的消息所能提供的平均信息量，是单个信源符号平均信息量的N倍。

*   **💡 注意点**：扩展信源是将原来的单个符号信源进行<font color="orange">分组</font>（或称为**块编码**的基础），把它看作一个新的、符号集更大的信源。这对于提高编码效率至关重要。

#### **🔄 离散无记忆信源编码与译码**

*   **唯一可译码**：若信源中每个不同的符号（或符号序列）编码后产生的码字都不同，则该码为<font color="orange">唯一可译码</font>。
    *   **必要条件**：若将 $L^J$ 个不同的信源符号序列，映射到 $D$ 进制、长度为 $n_J$ 的码字上，则码字的总数不能少于信源序列的总数。
        $$
        D^{n_J} \ge L^J
        $$

*   **编码速率 R**：表示编码一个信源符号所需的平均码元数（或信息量）。
    *   等长编码：$R = \frac{n_J}{J} \log_2 D$
    *   不等长编码：$R = \frac{\bar{n}_J}{J} \log_2 D$
    （其中 $n_J$ 是码长，$\bar{n}_J$ 是平均码长，J 是信源符号分组长度，D 是码元进制数）

*   **编码效率** $\eta_c$：信源熵与编码速率的比值，衡量编码的有效性。
    $$
    \eta_c = \frac{H(S)}{R}
    $$
    *   **无信息丢失要求**：为了保证编码不丢失信息，编码速率必须大于或等于信源熵。
        $$
        R \ge H(S) \implies \eta_c \le 1
        $$

#### **📏 4.5.2 离散无记忆信源的等长编码**

<font color="orange">等长编码</font>是指对信源的每个符号（或符号组）都用长度相等的码字来表示。

*   **码字长度**：对于有 $L$ 个符号的信源，采用 $D$ 进制码元进行编码，为保证唯一可译，码长 $n_1$ 必须满足：
    $$
    n_1 \ge \frac{\log_2 L}{\log_2 D}
    $$
    由于码长必须是整数，所以 $n_1 = \lceil \frac{\log_2 L}{\log_2 D} \rceil$。

*   **扩展编码 (联合编码)**：将 $J$ 个信源符号分为一组进行联合编码。
    *   随着分组长度 $J$ 的增大，每个信源符号所需的平均码元数 $n_1 = n_J/J$ 会更接近理论下限，编码效率得以提高。
        $$
        \lim_{J\to\infty} n_1 = \frac{\log_2 L}{\log_2 D}
        $$

*   **信源统计特性的影响**：对于同样的信源符号数和编码方式，概率分布更<font color="orange">不均匀</font>的信源，其熵 $H(S)$ 更小，即使编码速率相同，其编码效率 $\eta_c$ 也会更低，意味着冗余度更高。

#### **📉 有损等长编码与信源划分定理**

无损等长编码为了逼近理论极限，往往需要极大的分组长度 $J$，这在实际中难以应用。因此，可以考虑在允许微小失真的前提下进行<font color="orange">有损编码</font>。

##### **ε-典型序列集**

基于大数定律，当序列长度 $J$ 足够大时，序列的平均信息量会大概率地收敛于信源的熵 $H(S)$。

*   **定义**：<font color="orange">ε-典型序列集</font> $T_S(J, \varepsilon)$ 是指所有长度为 $J$ 的序列 $\bar{X}_J$ 中，满足下列条件的序列的集合：
    $$
T_S(J, \varepsilon) = \left\\{ \bar{X}\_J : H(S) - \varepsilon \le \frac{I(\bar{X}\_J)}{J} \le H(S) + \varepsilon \right\\}
$$
    其中 $I(\bar{X}_J) = -\log P(\bar{X}_J)$ 是序列 $\bar{X}_J$ 的自信息量。

##### **信源划分定理 (AEP)**

当 $J$ 足够大时，信源输出的所有序列可以被划分为两个集合：
1.  **典型序列集** $T_S(J, \varepsilon)$：
    *   **高概率集** ✅：虽然序列数量在总空间中占比极小，但其出现的总概率趋近于1。
        $$
        P[\bar{X}_J \in T_S(J, \varepsilon)] \ge 1 - \varepsilon
        $$
    *   **近似等概** ⚖️：集合中的每个序列出现的概率都非常接近 $2^{-JH(S)}$。
    *   **数量** 🔢：集合中序列的数量约为 $2^{JH(S)}$。

2.  **非典型序列集** $\bar{T}_S(J, \varepsilon)$：
    *   **低概率集** ❌：包含绝大多数可能的序列，但其出现的总概率趋近于0。

**核心思想**：我们只对出现概率极高的<font color="orange">典型序列</font>进行编码，而忽略掉几乎不会出现的非典型序列。这样，我们只需要约 $2^{JH(S)}$ 个码字，大大缩短了码长，从而提高效率。这种忽略小概率事件的做法会带来微小的错误概率（失真）。

##### **信源编码定理**

*   **定理**：对于一个熵为 $H(S)$ 的信源：
    *   若编码速率 $R \ge H(S)$，则速率 $R$ 是<font color="orange">可达的</font>（存在一种编码方式，使译码错误率任意小）。
    *   若编码速率 $R < H(S)$，则速率 $R$ 是<font color="orange">不可达的</font>。

*   **物理意义**：信源的<font color="orange">熵 H(S) 是无失真数据压缩的理论极限</font>。平均每个信源符号至少需要 $H(S)$ 比特的信息来表示。

#### **✒️ 4.5.3 离散无记忆信源的不等长编码**

基本思想：为概率大的事件分配<font color="orange">短码字</font>，为概率小的事件分配<font color="orange">长码字</font>。
$$
\text{平均码长 } \bar{n} = \sum_{k=1}^{K} n_k p(a_k)
$$
目标是使 $\bar{n}$ 尽可能小。

##### **唯一可译性与码的分类**

*   **唯一可译性**：任意码字序列的切分方式是唯一的。
*   **异字头码 (前缀码)** ✅：任何一个码字都不是其他任何码字的<font color="orange">前缀</font>。
    *   **优点**：具有**即时性**，即收到一个完整的码字后可以立即译码，无需等待后续码元。
    *   **注意**：异字头码一定是唯一可译码，但唯一可译码不一定是异字头码。

##### **码树 🌳**

码树是分析和构造前缀码的有力工具。
*   **树根**：码字的起点。
*   **分支**：代表码元（如二进制码的分支为0和1）。
*   **节点**：代表一个码字前缀。
*   **端节点**：代表一个完整的码字。

**前缀码的码树特性**：所有码字都位于码树的<font color="orange">端节点</font>上，没有任何一个码字是另一个码字的中间节点。

#### **📜 不等长编码的基本定理**

##### **1. Kraft 不等式**

对于一个有 $L$ 个信源符号，$D$ 进制的码，其码长为 $n_1, n_2, \dots, n_L$。
*   **定理**：存在一个具有这样码长组合的<font color="orange">异字头码</font>的**充要条件**是：
    $$
    \boxed{\sum_{i=1}^{L} D^{-n_i} \le 1}
    $$
*   **推论**：任何唯一可译码都满足Kraft不等式。因此，任何唯一可译码都可以被一个具有相同码长分布的异字头码所替代。

##### **2. 不等长编码定理**

对于任何一个唯一可译码，其平均码长 $\bar{n}$ 存在一个下界：
$$
\boxed{\bar{n} \ge \frac{H(S)}{\log D}}
$$
并且，总能找到一种编码方式，使其平均码长满足：
$$
\bar{n} < \frac{H(S)}{\log D} + 1
$$
*   **💡 易错点**：这里的 $H(S)$ 的对数底是2，而 $\log D$ 的底没有强制要求，但通常也取2，此时分母变为1，下界就是 $H(S)$。公式表达的是编码一个符号平均需要多少个D进制码元。

*   **联合编码**：与等长编码类似，通过对 $J$ 个符号进行联合编码，可以使每个符号的平均码长 $\bar\{n\}\_1$ 无限逼近理论下限。
    $$
    \lim_{J\to\infty} \bar{n}_1 = \frac{H(S)}{\log D}
    $$

#### **✨ 霍夫曼(Huffman)编码**

霍夫曼编码是一种构造<font color="orange">最佳不等长编码</font>（即平均码长最短的前缀码）的算法。

##### **编码步骤 (D进制)**

1.  **排序** 📊：将 $L$ 个信源符号按概率从大到小排列。
2.  **缩减** 🤏：
    *   选取概率最小的 $D$ 个符号。
    *   将这 $D$ 个符号的概率相加，得到一个“合并符号”的概率。
    *   为这 $D$ 个符号分别指定码元 $C_1, C_2, \dots, C_D$。
    *   用这个“合并符号”替代原来的 $D$ 个符号，得到一个只有 $L-D+1$ 个符号的新信源（称为缩减信源）。
3.  **迭代** 🔁：对缩减信源重复步骤1和2，直到最后只剩下 $D$ 个符号。
4.  **赋码** ➡️：为最后的 $D$ 个符号赋码。
5.  **回溯** ⬅️：从最后一步开始，沿合并路径反向回溯。每个符号的码字由其在回溯路径上遇到的所有码元<font color="orange">依次连接</font>而成（后遇到的码元放在前面）。

*   **💡 注意点 (非整数倍情况)**：如果初始信源符号数 $L$ 不能满足 $L = k(D-1)+D$（k为缩减次数），则需要添加 $m$ 个概率为0的<font color="orange">虚假符号</font>，使得新的符号总数 $L'$ 满足该条件。

##### **码长方差**

在平均码长 $\bar{n}$ 相同的情况下，码长的<font color="orange">方差</font>越小，码长分布越均匀，对缓冲区的要求越低。
$$
\sigma_C^2 = E[(n_i - \bar{n})^2] = \sum_{i=1}^{L} p(S_i)(n_i - \bar{n})^2
$$
*   **优化技巧**：在霍夫曼编码的排序过程中，当出现概率相同的符号时，应将“合并”得到的新符号放在排序列表的<font color="orange">最上方</font>。这样做可以有效减小码长的方差。

---

#### **💡 本章学习总结**

本节课我们系统学习了信源编码的基本理论和方法。

1.  **核心目标**：信源编码的核心是**消除信源冗余**以进行有效的数据压缩。冗余主要来源于符号间的相关性（记忆性）和概率分布的不均匀性。
2.  **理论极限**：信源的**熵 $H(S)$** 是无损压缩的理论极限。任何编码的速率 $R$ 必须大于等于 $H(S)$ 才能保证信息不丢失。
3.  **两大编码方式**：
    *   **等长编码**：实现简单，但效率通常不高。通过**扩展信源（分组）**可以提高效率。**信源划分定理**和**典型序列**的概念解释了有损等长编码的原理，即只编码高概率事件集。
    *   **不等长编码**：通过为高频符号分配短码字来提高效率，其性能通常优于等长编码。
4.  **关键概念与工具**：
    *   **前缀码（异字头码）** 是实现不等长编码的关键，它保证了码的即时可译性。
    *   **Kraft 不等式** 给出了前缀码存在的数学条件。
    *   **码树** 是设计和分析前缀码的可视化工具。
5.  **实用算法**：
    *   **霍夫曼编码** 是构造**最优前缀码**（平均码长最短）的经典算法。我们不仅要掌握其编码步骤，还要理解如何通过添加虚假符号来处理一般情况，以及如何通过排序策略来优化码长的方差。

总而言之，信源编码的本质是在<font color="orange">效率</font>（更短的码长）和<font color="orange">可靠性</font>（唯一可译/低失真）之间寻找最佳平衡，而信息论为我们提供了衡量这种平衡的理论基础和实现最优编码的指导方法。