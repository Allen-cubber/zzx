---
title: 信息论习题答案
tags:
  - 大三上
  - 信息论
date: 2025-11-16T08:00:00+08:00
summary: 习题答案
draft: true
---
### **4.1**

#### **题目**

某一信源以概率 $1/2$、$1/4$、$1/8$、$1/16$、$1/32$ 和 $1/32$ 产生 $6$ 种不同的符号 $x_1$、$x_2$、$x_3$、$x_4$、$x_5$ 和 $x_6$，每个符号出现是独立的，符号速率为 $1000$ (符号)/秒。
(1) 请计算每个符号所含的信息量；
(2) 求信源的熵；
(3) 求单位时间内输出的平均信息量。

---

#### **核心公式**

1.  **自信息量 (Self-information):**
    一个符号 $x_i$ 所包含的信息量由其概率 $P(x_i)$ 决定。
    $\boxed{I(x_i) = -\log_2 P(x_i)}$
    单位：比特 (bit)。

2.  **信源熵 (Source Entropy):**
    信源的平均信息量，是所有符号自信息量的统计期望。
    $\boxed{H(X) = E[I(X)] = -\sum_{i} P(x_i) \log_2 P(x_i)}$
    单位：比特/符号 (bit/symbol)。

3.  **信息速率 (Information Rate):**
    单位时间内信源输出的平均信息量。
    $\boxed{R = H(X) \times R_s}$
    其中 $R_s$ 是符号速率。单位：比特/秒 (bps)。

---

#### **解题步骤**

##### **(1) 计算每个符号所含的信息量**

根据自信息量公式 $I(x_i) = -\log_2 P(x_i)$，我们对每个符号进行计算：

*   对于符号 $x_1$，$P(x_1) = 1/2$：
    $I(x_1) = -\log_2(\frac{1}{2}) = \log_2(2) = 1$ 比特

*   对于符号 $x_2$，$P(x_2) = 1/4$：
    $I(x_2) = -\log_2(\frac{1}{4}) = \log_2(4) = 2$ 比特

*   对于符号 $x_3$，$P(x_3) = 1/8$：
    $I(x_3) = -\log_2(\frac{1}{8}) = \log_2(8) = 3$ 比特

*   对于符号 $x_4$，$P(x_4) = 1/16$：
    $I(x_4) = -\log_2(\frac{1}{16}) = \log_2(16) = 4$ 比特

*   对于符号 $x_5$，$P(x_5) = 1/32$：
    $I(x_5) = -\log_2(\frac{1}{32}) = \log_2(32) = 5$ 比特

*   对于符号 $x_6$，$P(x_6) = 1/32$：
    $I(x_6) = -\log_2(\frac{1}{32}) = \log_2(32) = 5$ 比特

##### **(2) 求信源的熵**

根据信源熵公式 $H(X) = \sum_{i} P(x_i) I(x_i)$，代入各符号的概率和信息量：
$H(X) = P(x_1)I(x_1) + P(x_2)I(x_2) + P(x_3)I(x_3) + P(x_4)I(x_4) + P(x_5)I(x_5) + P(x_6)I(x_6)$
$H(X) = (\frac{1}{2} \times 1) + (\frac{1}{4} \times 2) + (\frac{1}{8} \times 3) + (\frac{1}{16} \times 4) + (\frac{1}{32} \times 5) + (\frac{1}{32} \times 5)$
$H(X) = \frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{4}{16} + \frac{10}{32}$
$H(X) = \frac{16}{32} + \frac{16}{32} + \frac{12}{32} + \frac{8}{32} + \frac{10}{32}$
$H(X) = \frac{16 + 16 + 12 + 8 + 10}{32} = \frac{62}{32} = \frac{31}{16}$
$H(X) = 1.9375 \text{ 比特/符号}$

##### **(3) 求单位时间内输出的平均信息量**

根据信息速率公式 $R = H(X) \times R_s$，代入信源熵和符号速率：
*   信源熵 $H(X) = 1.9375$ 比特/符号
*   符号速率 $R_s = 1000$ 符号/秒

$R = 1.9375 \text{ (比特/符号)} \times 1000 \text{ (符号/秒)}$
$R = 1937.5 \text{ 比特/秒}$

---

#### **答案**

1.  **每个符号所含的信息量：**
    *   $I(x_1)=1$ 比特
    *   $I(x_2)=2$ 比特
    *   $I(x_3)=3$ 比特
    *   $I(x_4)=4$ 比特
    *   $I(x_5)=5$ 比特
    *   $I(x_6)=5$ 比特

2.  **信源的熵：** $1.9375$ 比特/符号。

3.  **单位时间内输出的平均信息量：** $1937.5$ 比特/秒。
### 4.2

#### 题目

一个离散信号源每毫秒发出 4 种符号中的一个，各相互独立符号出现的概率分别为 0.4、0.3、0.2 和 0.1，求该信号源的平均信息量与信息速率。

---

#### 核心公式

1.  **离散信源的平均信息量（信息熵）**
    对于一个包含 M 个独立符号 $x_1, x_2, \dots, x_M$ 的离散信源，其平均信息量 $H(X)$ 的计算公式为：
    $$
    \boxed{H(X) = -\sum_{i=1}^{M} p(x_i) \log_2 p(x_i)}
    $$
    其中，$p(x_i)$ 是符号 $x_i$ 出现的概率。单位为：比特/符号 (bit/symbol)。

2.  **信息速率**
    信息速率 $R_b$ 是指单位时间内传输的平均信息量，其计算公式为：
    $$
    \boxed{R_b = r_s \cdot H(X)}
    $$
    其中，$r_s$ 是码元速率（或符号速率），单位为波特 (Baud)；$H(X)$ 是信源的平均信息量。信息速率的单位为：比特/秒 (bit/s 或 bps)。

---

#### 解题步骤

根据题目所给条件，我们可以进行如下步骤计算：

1.  **计算信源的平均信息量（信息熵）$H(X)$**
    该信源有 4 种符号，其出现的概率分别为：
    $p(x_1) = 0.4$
    $p(x_2) = 0.3$
    $p(x_3) = 0.2$
    $p(x_4) = 0.1$

    将这些概率代入信息熵的公式中：
    $$
    \begin{aligned}
    H(X) &= -\sum_{i=1}^{4} p(x_i) \log_2 p(x_i) \\\\
    &= -[p(x_1)\log_2 p(x_1) + p(x_2)\log_2 p(x_2) + p(x_3)\log_2 p(x_3) + p(x_4)\log_2 p(x_4)] \\\\
    &= -[0.4 \log_2 0.4 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1] \\\\
    &= -[0.4 \times (-1.322) + 0.3 \times (-1.737) + 0.2 \times (-2.322) + 0.1 \times (-3.322)] \\\\
    &= -[-0.5288 - 0.5211 - 0.4644 - 0.3322] \\\\
    &= -[-1.8465] \\\\
    &= 1.8465 \text{ 比特/符号}
    \end{aligned}
    $$
    所以，该信号源的平均信息量为 1.8465 比特/符号。

2.  **计算信息速率 $R_b$**
    首先，需要确定符号速率 $r_s$。题目中给出，信源每毫秒发出一个符号，即每个符号的持续时间为 $T_s = 1 \text{ ms} = 1 \times 10^{-3} \text{ s}$。
    符号速率 $r_s$ 是符号持续时间的倒数：
    $$
    r_s = \frac{1}{T_s} = \frac{1}{1 \times 10^{-3} \text{ s}} = 1000 \text{ 符号/秒} = 1000 \text{ Baud}
    $$
    然后，将平均信息量 $H(X)$ 和符号速率 $r_s$ 代入信息速率公式：
    $$
    \begin{aligned}
    R_b &= r_s \cdot H(X) \\\\
    &= 1000 \text{ Baud} \times 1.8465 \text{ 比特/符号} \\\\
    &= 1846.5 \text{ 比特/秒 (bps)}
    \end{aligned}
    $$
    所以，该信号源的信息速率为 1846.5 bps。

---

#### 答案

该信号源的平均信息量为 **1.8465 比特/符号**，信息速率为 **1846.5 bps**。

### 4.3

#### 题目

设有 4 个消息符号，其出现的概率分别是 1/8、1/8、1/4 和 1/2，各消息符号的出现是相对独立的，求该符号集的平均信息量。

---

#### 核心公式

**离散信源的平均信息量（信息熵）**
对于一个包含 M 个独立符号 $x_1, x_2, \dots, x_M$ 的离散信源，其平均信息量 $H(X)$ 的计算公式为：
$$
\boxed{H(X) = -\sum_{i=1}^{M} p(x_i) \log_2 p(x_i)}
$$
其中，$p(x_i)$ 是符号 $x_i$ 出现的概率。单位为：比特/符号 (bit/symbol)。

---

#### 解题步骤

根据题目所给条件，我们可以进行如下步骤计算：

1.  **确定各符号的概率**
    该符号集有 4 个消息符号，其出现的概率分别为：
    $p(x_1) = 1/8$
    $p(x_2) = 1/8$
    $p(x_3) = 1/4$
    $p(x_4) = 1/2$
    

2.  **计算信源的平均信息量（信息熵）$H(X)$**
    将这些概率代入信息熵的公式中：
    $$
    \begin{aligned}
    H(X) &= -\sum_{i=1}^{4} p(x_i) \log_2 p(x_i) \\
    &= -[p(x_1)\log_2 p(x_1) + p(x_2)\log_2 p(x_2) + p(x_3)\log_2 p(x_3) + p(x_4)\log_2 p(x_4)] \\
    &= -[\frac{1}{8} \log_2 \frac{1}{8} + \frac{1}{8} \log_2 \frac{1}{8} + \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{2} \log_2 \frac{1}{2}] \\
    \end{aligned}
    $$
    由于 $\log_2 \frac{1}{2^n} = -n$，我们可以得到：
    $\log_2 \frac{1}{8} = \log_2 2^{-3} = -3$
    $\log_2 \frac{1}{4} = \log_2 2^{-2} = -2$
    $\log_2 \frac{1}{2} = \log_2 2^{-1} = -1$

    将上述结果代入计算：
    $$
    \begin{aligned}
    H(X) &= -[\frac{1}{8} \times (-3) + \frac{1}{8} \times (-3) + \frac{1}{4} \times (-2) + \frac{1}{2} \times (-1)] \\
    &= -[-\frac{3}{8} - \frac{3}{8} - \frac{2}{4} - \frac{1}{2}] \\
    &= -[-\frac{6}{8} - \frac{1}{2} - \frac{1}{2}] \\
    &= -[-\frac{3}{4} - 1] \\
    &= -[-\frac{7}{4}] \\
    &= \frac{7}{4} \\
    &= 1.75 \text{ 比特/符号}
    \end{aligned}
    $$
    所以，该符号集的平均信息量为 1.75 比特/符号。

---

#### 答案

该符号集的平均信息量为 **1.75 比特/符号**。

### 4.4

#### 题目

计算字母集的信息熵。(1) 若把英文的 26 个字母和空格共 27 个符号，视为等概出现，求其信息熵；(2) 若英文字母和空格的概率分布如题 4.4 表所示，求其信息熵。

**题 4.4 表 英文字母的概率分布**

| 字母 | 空格 | E | T | O | A | N | I | R | S |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **概率** | 0.1956 | 0.1050 | 0.0720 | 0.0654 | 0.0630 | 0.0590 | 0.0550 | 0.0540 | 0.0520 |
| **字母** | H | D | L | C | F | U | M | P | Y |
| **概率** | 0.0470 | 0.0350 | 0.0290 | 0.0230 | 0.0225 | 0.0225 | 0.0210 | 0.0175 | 0.0120 |
| **字母** | W | G | B | V | K | X | J | Q | Z |
| **概率** | 0.0120 | 0.0110 | 0.0105 | 0.0080 | 0.0030 | 0.0020 | 0.0010 | 0.0010 | 0.0010 |

---

#### 核心公式

1.  **离散信源的一般信息熵**
    对于一个包含 M 个独立符号 $x_1, x_2, \dots, x_M$ 的离散信源，其信息熵 $H(X)$ 的计算公式为：
    $$
    \boxed{H(X) = -\sum_{i=1}^{M} p(x_i) \log_2 p(x_i)}
    $$
    其中，$p(x_i)$ 是符号 $x_i$ 出现的概率。单位为：比特/符号 (bit/symbol)。

2.  **等概分布信源的信息熵**
    当信源中的 M 个符号是等概率出现时，即 $p(x_i) = 1/M$，信息熵达到最大值，计算公式简化为：
    $$
    \boxed{H(X)_{\max} = \log_2 M}
    $$

---

#### 解题步骤

**(1) 若 27 个符号等概出现**

1.  **确定参数**
    符号总数 $M = 27$ (26个英文字母 + 1个空格)。
    由于是等概出现，每个符号的概率为 $p(x_i) = \frac{1}{M} = \frac{1}{27}$。

2.  **计算信息熵**
    使用等概分布信源的信息熵公式：
    $$
    \begin{aligned}
    H(X) &= \log_2 M \\
    &= \log_2 27 \\
    &\approx 4.7549 \text{ 比特/符号}
    \end{aligned}
    $$
    因此，当 27 个符号等概出现时，信息熵约为 4.7549 比特/符号。

**(2) 若符号按题 4.4 表的概率分布出现**

1.  **确定参数**
    符号总数 $M = 27$。
    每个符号的概率 $p(x_i)$ 由表格给出，例如 $p(\text{空格})=0.1956, p(E)=0.1050, \dots, p(Z)=0.0010$。

2.  **计算信息熵**
    使用离散信源的一般信息熵公式，将表格中所有 27 个概率值代入：
    $$
    H(X) = -\sum_{i=1}^{27} p(x_i) \log_2 p(x_i)
    $$
	根据给定的概率分布，该字母集的信息熵约为 4.0831 比特/符号。

---

#### 答案

1.  若 27 个符号（26个英文字母和空格）等概出现，其信息熵为 **4.7549 比特/符号**。
2.  若符号按题 4.4 表给出的概率分布出现，其信息熵为 **4.0831 比特/符号**。

**结论**：可以看出，当信源符号的概率分布不均匀时，其信息熵（平均不确定性）小于等概率分布情况下的最大信息熵。

### 4.5

#### 题目

中文电码表采用四位阿拉伯数字作为代号，假定这种数字代码出现的概率如题 4.5 表所示，如果一个报文中包含了 10000 个中文字，试估计该报文最多可包含多少信息量。

**题 4.5 表 汉字报文中数字代码的出现概率**

| 数字 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **概率** | 0.260 | 0.160 | 0.080 | 0.062 | 0.060 | 0.063 | 0.155 | 0.062 | 0.048 | 0.052 |

---

#### 核心公式

1.  **离散信源的平均信息量（信息熵）**
    信息熵 $H(X)$ 表示信源每个符号所含的平均信息量。
    $$
    \boxed{H(X) = -\sum_{i=1}^{M} p(x_i) \log_2 p(x_i)}
    $$
    其中，$M$ 是符号的总数，$p(x_i)$ 是符号 $x_i$ 出现的概率。单位为：比特/符号 (bit/symbol)。

2.  **总信息量**
    对于一个包含 N 个独立符号的序列，其总信息量 $I_{total}$ 为：
    $$
    \boxed{I_{total} = N \cdot H(X)}
    $$
    其中，$N$ 是序列中符号的总个数，$H(X)$ 是每个符号的平均信息量。

---

#### 解题步骤

本题的目标是计算一个包含 10000 个汉字的报文的总信息量。解题思路是：首先计算出单个阿拉伯数字的平均信息量（信息熵），然后计算出整个报文包含的阿拉伯数字总数，最后将两者相乘得到总信息量。

1.  **计算单个数字的平均信息量 $H(D)$**
    这里的信源符号是阿拉伯数字 0 到 9，共 $M=10$ 个。根据题 4.5 表给出的概率，我们应用信息熵公式：
    $$
    H(D) = -\sum_{i=0}^{9} p(i) \log_2 p(i)
    $$
	 每个阿拉伯数字平均携带约 3.059 比特的信息量。

2.  **计算报文中数字的总数 N**
    每个汉字由 4 位阿拉伯数字表示，报文共有 10000 个汉字。
    $$
    N = 10000 \text{ (个汉字)} \times 4 \text{ (个数字/汉字)} = 40000 \text{ (个数字)}
    $$
    整个报文由 40000 个阿拉伯数字组成。

3.  **计算报文的总信息量 $I_{total}$**
    将数字总数 N 和单个数字的平均信息量 $H(D)$ 相乘：
    $$
    \begin{aligned}
    I_{total} &= N \cdot H(D) \\
    &= 40000 \text{ (个数字)} \times 3.059 \text{ (比特/数字)} \\
    &= 122360 \text{ 比特}
    \end{aligned}
    $$

---

#### 答案

该报文最多可包含的信息量约为 **122360 比特**。

### 4.6

#### 题目

证明平均互信息量的互易性，即 $I(X;Y) = I(Y;X)$。

---

#### 核心公式

1.  **平均互信息量 (Average Mutual Information)**
    平均互信息量 $I(X;Y)$ 衡量了两个随机变量 $X$ 和 $Y$ 之间的相互依赖性。其定义式为：
    $$
    \boxed{
    I(X;Y) = \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i, y_j) \log_2 \frac{p(x_i, y_j)}{p(x_i)p(y_j)}
    }
    $$
    其中 $p(x_i, y_j)$ 是联合概率，$p(x_i)$ 和 $p(y_j)$ 分别是边缘概率。

2.  **信息熵与条件熵的关系**
    平均互信息量也可以通过信息熵和条件熵来表示：
    $$
    I(X;Y) = H(X) - H(X|Y) \\\\
    I(Y;X) = H(Y) - H(Y|X)
    $$

3.  **概率论基本关系**
    联合概率与条件概率的关系：
    $$
    p(x_i, y_j) = p(x_i|y_j)p(y_j) = p(y_j|x_i)p(x_i)
    $$
    联合概率的对称性：
    $$
    p(x_i, y_j) = p(y_j, x_i)
    $$

---

#### 证明过程

**证法一：利用平均互信息量的定义式**

根据平均互信息量的定义，我们有：
$$
I(X;Y) = \sum_{i} \sum_{j} p(x_i, y_j) \log_2 \frac{p(x_i, y_j)}{p(x_i)p(y_j)}
$$
同样地，我们可以写出 $I(Y;X)$ 的定义式：
$$
I(Y;X) = \sum_{j} \sum_{i} p(y_j, x_i) \log_2 \frac{p(y_j, x_i)}{p(y_j)p(x_i)}
$$
由于联合概率具有对称性，即 $p(x_i, y_j) = p(y_j, x_i)$，并且求和的顺序可以交换，我们可以将 $I(Y;X)$ 的表达式改写为：
$$
I(Y;X) = \sum_{i} \sum_{j} p(x_i, y_j) \log_2 \frac{p(x_i, y_j)}{p(x_i)p(y_j)}
$$
观察可得，上式与 $I(X;Y)$ 的定义式完全相同。
因此，
$$
I(X;Y) = I(Y;X)
$$
证毕。

**证法二：利用信息熵和条件熵的关系**

根据信息熵、条件熵和联合熵的关系，我们有：
$$
H(X,Y) = H(X) + H(Y|X)
$$
$$
H(X,Y) = H(Y) + H(X|Y)
$$
因此，
$$
H(X) + H(Y|X) = H(Y) + H(X|Y)
$$
通过移项整理，可以得到：
$$
H(X) - H(X|Y) = H(Y) - H(Y|X)
$$
根据平均互信息量与信息熵、条件熵的关系式：
$$
I(X;Y) = H(X) - H(X|Y)
$$
$$
I(Y;X) = H(Y) - H(Y|X)
$$
代入整理后的等式，即可证明：
$$
I(X;Y) = I(Y;X)
$$
证毕。

好的，这是对该题目的详细解析。

### 4.7

#### 题目

证明两离散信源的条件熵和熵之间满足关系式 $I(X;Y) = H(Y) - H(Y|X)$。

---

#### 核心公式

1.  **平均互信息量 (Average Mutual Information)**
    $$
    \boxed{
    I(X;Y) = \sum_{i} \sum_{j} p(x_i, y_j) \log_2 \frac{p(x_i | y_j)}{p(x_i)}
    }
    $$
    或等价地：
    $$
    \boxed{
    I(X;Y) = \sum_{i} \sum_{j} p(x_i, y_j) \log_2 \frac{p(y_j | x_i)}{p(y_j)}
    }
    $$

2.  **信息熵 (Entropy)**
    $$
    \boxed{
    H(Y) = -\sum_{j} p(y_j) \log_2 p(y_j)
    }
    $$

3.  **条件熵 (Conditional Entropy)**
    $$
    \boxed{
    H(Y|X) = -\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i)
    }
    $$

4.  **概率论基本关系**
    *   联合概率与条件概率：$p(x_i, y_j) = p(y_j|x_i)p(x_i)$
    *   边缘概率：$p(y_j) = \sum_{i} p(x_i, y_j)$

---

#### 证明过程

从平均互信息量的定义出发进行推导。
$$
\begin{aligned}
I(X;Y) &= \sum_{i} \sum_{j} p(x_i, y_j) \log_2 \frac{p(y_j | x_i)}{p(y_j)} \\\\
\end{aligned}
$$
利用对数的性质 $\log(a/b) = \log(a) - \log(b)$，将上式展开：
$$
\begin{aligned}
I(X;Y) &= \sum_{i} \sum_{j} p(x_i, y_j) [\log_2 p(y_j | x_i) - \log_2 p(y_j)] \\\\
&= \sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i) - \sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j)
\end{aligned}
$$
现在我们分别分析等式右边的两项。

对于第一项：
$$
\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i)
$$
根据条件熵的定义 $H(Y|X) = -\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i)$，可知这一项等于 $-H(Y|X)$。

对于第二项：
$$
- \sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j)
$$
由于求和是对 $i$ 和 $j$ 进行的，而 $\log_2 p(y_j)$ 与 $i$ 无关，我们可以交换求和顺序：
$$
= - \sum_{j} \left( \sum_{i} p(x_i, y_j) \right) \log_2 p(y_j)
$$
根据边缘概率的定义，$\sum_{i} p(x_i, y_j) = p(y_j)$。因此，上式变为：
$$
= - \sum_{j} p(y_j) \log_2 p(y_j)
$$
根据信息熵的定义，这一项正好等于 $H(Y)$。

将分析得到的两项结果代回原式，可得：
$$
I(X;Y) = (-H(Y|X)) + H(Y)
$$
整理后即为：
$$
I(X;Y) = H(Y) - H(Y|X)
$$
证毕。

### 4.8

#### 题目

若 X 与 Y 统计独立，证明 $H(Y|X) = H(Y)$。

---

#### 核心公式

1.  **条件熵 (Conditional Entropy)**
    条件熵 $H(Y|X)$ 表示在已知随机变量 X 的条件下，随机变量 Y 的不确定性。其定义式为：
    $$
    \boxed{
    H(Y|X) = -\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i)
    }
    $$

2.  **统计独立 (Statistical Independence)**
    当 X 和 Y 统计独立时，它们的条件概率等于边缘概率，联合概率等于边缘概率的乘积：
    $$
    \boxed{
    p(y_j|x_i) = p(y_j) \\\\
    p(x_i, y_j) = p(x_i)p(y_j)
    }
    $$

3.  **信息熵 (Entropy)**
    信息熵 $H(Y)$ 表示随机变量 Y 的平均不确定性。
    $$
    \boxed{
    H(Y) = -\sum_{j} p(y_j) \log_2 p(y_j)
    }
    $$

---

#### 证明过程

从条件熵的定义出发：
$$
H(Y|X) = -\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j | x_i)
$$
根据题目条件，随机变量 X 与 Y 是统计独立的。因此，我们可以利用统计独立的性质进行代换：
$$
p(y_j | x_i) = p(y_j)
$$
将此关系代入条件熵的定义式中，得到：
$$
H(Y|X) = -\sum_{i} \sum_{j} p(x_i, y_j) \log_2 p(y_j)
$$
由于 $\log_2 p(y_j)$ 项不随索引 $i$ 变化，我们可以将其提取到关于 $i$ 的求和之外：
$$
\begin{aligned}
H(Y|X) &= - \sum_{j} \left( \sum_{i} p(x_i, y_j) \right) \log_2 p(y_j) \\\\
\end{aligned}
$$
根据边缘概率的定义，$\sum_{i} p(x_i, y_j) = p(y_j)$。将此代入上式：
$$
H(Y|X) = - \sum_{j} p(y_j) \log_2 p(y_j)
$$
观察上式右边，它正是信息熵 $H(Y)$ 的定义。因此，我们得出结论：
$$
H(Y|X) = H(Y)
$$
证毕。

**结论**：该证明的物理意义是，如果两个变量是统计独立的，那么知道其中一个变量 (X) 的取值，并不会减少另一个变量 (Y) 的不确定性。因此，Y 在 X 已知条件下的条件熵等于 Y 本身的熵。

### 4.9

#### 题目

证明，一般地有 $H(X|Y) \ge 0$。

---

#### 核心公式

1.  **条件熵 (Conditional Entropy)**
    条件熵 $H(X|Y)$ 可以表示为在 Y 的不同取值条件下 X 的熵的加权平均：
    $$
    \boxed{
    H(X|Y) = \sum_{j} p(y_j) H(X|Y=y_j)
    }
    $$
    其中，$H(X|Y=y_j)$ 是在给定 $Y=y_j$ 的条件下 X 的信息熵，其定义为：
    $$
    H(X|Y=y_j) = -\sum_{i} p(x_i|y_j) \log_2 p(x_i|y_j)
    $$

2.  **信息熵的非负性**
    对于任意一个随机变量 Z，其信息熵 $H(Z)$ 总是大于等于 0。
    $$
    \boxed{
    H(Z) = -\sum_{k} p(z_k) \log_2 p(z_k) \ge 0
    }
    $$
    这是因为概率 $p(z_k)$ 的取值范围是 $[0, 1]$，所以 $\log_2 p(z_k) \le 0$，从而 $-p(z_k) \log_2 p(z_k) \ge 0$。

---

#### 证明过程

根据条件熵的定义，我们有：
$$
H(X|Y) = \sum_{j} p(y_j) H(X|Y=y_j)
$$
我们来分析这个求和式中的每一项。

1.  **第一部分：$p(y_j)$**
    $p(y_j)$ 是随机变量 Y 取值为 $y_j$ 的概率，根据概率的定义，必有 $p(y_j) \ge 0$。

2.  **第二部分：$H(X|Y=y_j)$**
    $H(X|Y=y_j)$ 是在 $Y=y_j$ 这个特定条件下，随机变量 X 的信息熵。它的计算公式为：
    $$
    H(X|Y=y_j) = -\sum_{i} p(x_i|y_j) \log_2 p(x_i|y_j)
    $$
    对于一个给定的 $y_j$，条件概率的集合 $\{p(x_1|y_j), p(x_2|y_j), \dots\}$ 构成了一个完整的概率分布，因为 $\sum_{i} p(x_i|y_j) = 1$。
    
    根据信息熵的非负性可知，任何一个有效的概率分布，其信息熵都大于等于零。因此，对于任意的 $y_j$，都有：
    $$
    H(X|Y=y_j) \ge 0
    $$

**综合分析**
条件熵 $H(X|Y)$ 是由多个项 $p(y_j) H(X|Y=y_j)$ 相加得到的。对于其中的每一项：
*   $p(y_j) \ge 0$
*   $H(X|Y=y_j) \ge 0$

两个非负数的乘积仍然是非负数，所以 $p(y_j) H(X|Y=y_j) \ge 0$。
$H(X|Y)$ 是对所有可能的 $j$ 的这些非负项求和，因此其结果也必然是非负的。
$$
H(X|Y) = \sum_{j} p(y_j) H(X|Y=y_j) \ge 0
$$
证毕。

**结论**：条件熵代表了在已知一个变量后，另一个变量仍然存在的不确定性。不确定性度量本身不可能是负值，最小为零（当一个变量完全确定另一个变量时）。

### 4.10

#### 题目

证明，一般地有 $H(X|Y) \le H(X)$ 和 $H(Y|X) \le H(Y)$。

---

#### 核心公式

1.  **平均互信息量与熵的关系**
    平均互信息量可以由信息熵和条件熵表示：
    $$
    \boxed{
    I(X;Y) = H(X) - H(X|Y) \\\\
    I(Y;X) = H(Y) - H(Y|X)
    }
    $$

2.  **平均互信息量的非负性 (信息不等式)**
    任意两个随机变量之间的平均互信息量总是大于等于零。
    $$
    \boxed{
    I(X;Y) \ge 0
    }
    $$
    等号成立的条件是当且仅当 X 和 Y 相互统计独立。

---

#### 证明过程

根据关系式：
$$
H(Y|X) = H(Y) - I(Y;X)
$$
根据平均互信息量的互易性（已在题 4.6 证明），有 $I(Y;X) = I(X;Y)$。
由于我们已经证明了 $I(X;Y) \ge 0$，因此 $I(Y;X) \ge 0$ 也成立。
所以：
$$
H(Y) - H(Y|X) \ge 0
$$
移项可得：
$$
H(Y) \ge H(Y|X)
$$

**结论**
该不等式的物理意义是：对一个随机变量的了解（条件的确立），不会增加另一个随机变量的不确定性。通常情况下，获取信息会减少不确定性。只有当两个变量相互独立时，获取一个变量的信息对另一个变量的不确定性没有影响，此时等号成立，即 $H(X|Y) = H(X)$。

### 4.11

#### 题目

已知非对称二进制信道，输入符号的概率场为
$$
\begin{pmatrix} x_1 & x_2 \\ p(x_1) & p(x_2) \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1/4 & 3/4 \end{pmatrix}
$$
信道转移概率矩阵为
$$
\begin{bmatrix} p(y_1/x_1) & p(y_2/x_1) \\ p(y_1/x_2) & p(y_2/x_2) \end{bmatrix} = \begin{bmatrix} p(0/0) & p(1/0) \\ p(0/1) & p(1/1) \end{bmatrix} = \begin{bmatrix} 0.8 & 0.2 \\ 0.1 & 0.9 \end{bmatrix}
$$
求：(1) 输入符号集 X 的平均信息量 $H(X)$；(2) 输出符号集 Y 的平均信息量 $H(Y)$；(3) 条件熵 $H(X|Y)$ 和 $H(Y|X)$；(4) 平均互信息量 $I(X,Y)$。

---

#### 核心公式

1.  **信息熵**
    $$
    \boxed{H(U) = -\sum_{i} p(u_i) \log_2 p(u_i)}
    $$

2.  **条件熵**
    $$
    \boxed{H(V|U) = \sum_{i} p(u_i) H(V|U=u_i) = -\sum_{i} \sum_{j} p(u_i, v_j) \log_2 p(v_j|u_i)}
    $$

3.  **联合概率与边缘概率**
    $$
    \boxed{p(u_i, v_j) = p(u_i)p(v_j|u_i) \\\\ p(v_j) = \sum_{i} p(u_i, v_j) = \sum_{i} p(u_i)p(v_j|u_i)}
    $$

4.  **平均互信息量**
    $$
    \boxed{I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)}
    $$

---

#### 解题步骤

根据题目所给条件，我们逐一进行计算。
**符号约定**: 输入符号 $x_1=0, x_2=1$；输出符号 $y_1=0, y_2=1$。
**已知概率**:
*   输入概率: $p(x_1) = p(0) = 1/4 = 0.25$, $p(x_2) = p(1) = 3/4 = 0.75$
*   转移概率:
    *   $p(y_1|x_1)=p(0|0)=0.8, \quad p(y_2|x_1)=p(1|0)=0.2$
    *   $p(y_1|x_2)=p(0|1)=0.1, \quad p(y_2|x_2)=p(1|1)=0.9$

**(1) 计算输入符号集的平均信息量 $H(X)$**
$$
\begin{aligned}
H(X) &= -\sum_{i=1}^{2} p(x_i) \log_2 p(x_i) \\
&= -[p(0) \log_2 p(0) + p(1) \log_2 p(1)] \\
&= -[0.25 \log_2 0.25 + 0.75 \log_2 0.75] \\
&= -[0.25 \times (-2) + 0.75 \times (-0.415)] \\
&= -[-0.5 - 0.3113] \\
&= 0.8113 \text{ 比特/符号}
\end{aligned}
$$

**(2) 计算输出符号集的平均信息量 $H(Y)$**
首先，需要计算输出符号的概率分布 $p(y_j)$。
$$
\begin{aligned}
p(y_1) = p(0) &= \sum_{i=1}^{2} p(x_i) p(y_1|x_i) \\
&= p(x_1)p(y_1|x_1) + p(x_2)p(y_1|x_2) \\
&= 0.25 \times 0.8 + 0.75 \times 0.1 \\
&= 0.2 + 0.075 = 0.275
\end{aligned}
$$
$$
\begin{aligned}
p(y_2) = p(1) &= \sum_{i=1}^{2} p(x_i) p(y_2|x_i) \\
&= p(x_1)p(y_2|x_1) + p(x_2)p(y_2|x_2) \\
&= 0.25 \times 0.2 + 0.75 \times 0.9 \\
&= 0.05 + 0.675 = 0.725
\end{aligned}
$$
（检查：$p(y_1) + p(y_2) = 0.275 + 0.725 = 1.0$，正确。）

现在计算 $H(Y)$：
$$
\begin{aligned}
H(Y) &= -\sum_{j=1}^{2} p(y_j) \log_2 p(y_j) \\
&= -[p(y_1) \log_2 p(y_1) + p(y_2) \log_2 p(y_2)] \\
&= -[0.275 \log_2 0.275 + 0.725 \log_2 0.725] \\
&= -[0.275 \times (-1.862) + 0.725 \times (-0.464)] \\
&= -[-0.5119 - 0.3364] \\
&= 0.8483 \text{ 比特/符号}
\end{aligned}
$$

**(3) 计算条件熵 $H(Y|X)$ 和 $H(X|Y)$**

先计算 $H(Y|X)$（信道损失熵）：
$$
\begin{aligned}
H(Y|X) &= \sum_{i=1}^{2} p(x_i) H(Y|x_i) \\
&= p(x_1) H(Y|x_1=0) + p(x_2) H(Y|x_2=1)
\end{aligned}
$$
其中，
$$
\begin{aligned}
H(Y|x_1=0) &= -[p(0|0)\log_2 p(0|0) + p(1|0)\log_2 p(1|0)] \\
&= -[0.8 \log_2 0.8 + 0.2 \log_2 0.2] \\
&= 0.7219 \text{ 比特/符号}
\end{aligned}
$$
$$
\begin{aligned}
H(Y|x_2=1) &= -[p(0|1)\log_2 p(0|1) + p(1|1)\log_2 p(1|1)] \\
&= -[0.1 \log_2 0.1 + 0.9 \log_2 0.9] \\
&= 0.4690 \text{ 比特/符号}
\end{aligned}
$$
所以，
$$
\begin{aligned}
H(Y|X) &= 0.25 \times 0.7219 + 0.75 \times 0.4690 \\
&= 0.1805 + 0.3518 \\
&= 0.5323 \text{ 比特/符号}
\end{aligned}
$$
$H(X|Y)$（信源疑义度）将在计算出互信息后求得。

**(4) 计算平均互信息量 $I(X,Y)$**
利用公式 $I(X;Y) = H(Y) - H(Y|X)$：
$$
\begin{aligned}
I(X;Y) &= H(Y) - H(Y|X) \\
&= 0.8483 - 0.5323 \\
&= 0.3160 \text{ 比特/符号}
\end{aligned}
$$
现在我们可以回头计算 $H(X|Y)$：
$$
\begin{aligned}
H(X|Y) &= H(X) - I(X;Y) \\
&= 0.8113 - 0.3160 \\
&= 0.4953 \text{ 比特/符号}
\end{aligned}
$$

---

#### 答案

综上所述，计算结果如下：
1.  **输入符号集的平均信息量 $H(X)$**：$0.8113$ 比特/符号
2.  **输出符号集的平均信息量 $H(Y)$**：$0.8483$ 比特/符号
3.  **条件熵**：
    *   $H(Y|X) = 0.5323$ 比特/符号
    *   $H(X|Y) = 0.4953$ 比特/符号
4.  **平均互信息量 $I(X,Y)$**：$0.3160$ 比特/符号